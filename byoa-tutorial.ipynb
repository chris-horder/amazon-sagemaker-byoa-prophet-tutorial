{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BYOA Tutorial - Prophet Forecasting en Sagemaker\n",
    "\n",
    "The following notebook shows how to integrate your own algorithms to Amazon Sagemaker. We are going to go the way of putting together an inference pipeline on the Prophet algorithm for time series. The algorithm is installed in a docker container and then it helps us to train the model and make inferences on an endpoint.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Assemble the dataset\n",
    "\n",
    "We are going to work with a public dataset that we must download from Kaggle. This dataset is called: Avocado Prices: Historical data on avocado prices and sales volume in multiple US markets and can be downloaded from: https://www.kaggle.com/neuromusic/avocado-prices/download Once downloaded, we must upload it to the same directory where we are running this notebook. The following code prepares the dataset so that Prophet can understand it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# We are left with only the date and the sales\n",
    "df = pd.read_csv('avocado.csv')\n",
    "df = df[['Date', 'AveragePrice']].dropna()\n",
    "\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df = df.set_index('Date')\n",
    "\n",
    "# We leave 1 single record per day with the average sales\n",
    "daily_df = df.resample('D').mean()\n",
    "d_df = daily_df.reset_index().dropna()\n",
    "\n",
    "# We format the column names as Prophet expects them\n",
    "d_df = d_df[['Date', 'AveragePrice']]\n",
    "d_df.columns = ['ds', 'y']\n",
    "d_df.head()\n",
    "\n",
    "# We save the resulting dataset as avocado_daily.csv\n",
    "d_df.to_csv(\"avocado_daily.csv\",index = False , columns = ['ds', 'y'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Package and Upload the Algorithm for Use with Amazon SageMaker\n",
    "\n",
    "\n",
    "### An overview of Docker\n",
    "\n",
    "Docker provides a simple way to package code into an _image_ that is completely self-contained. Once you have an image, you can use Docker to run a _container_ based on that image. Running a container is the same as running a program on the machine, except that the container creates a completely self-contained environment for the program to run. Containers are isolated from each other and from the host environment, so the way you configure the program is the way it runs, no matter where you run it.\n",
    "\n",
    "Docker is more powerful than environment managers like conda or virtualenv because (a) it is completely language independent and (b) it understands your entire operating environment, including startup commands, environment variables, etc.\n",
    "\n",
    "In some ways, a Docker container is like a virtual machine, but it is much lighter. For example, a program that runs in one container can start in less than a second, and many containers can run on the same physical machine or virtual machine instance.\n",
    "\n",
    "Docker uses a simple file called `Dockerfile` to specify how the image is assembled.\n",
    "Amazon SagMaker uses Docker to allow users to train and implement algorithms.\n",
    "\n",
    "In Amazon SageMaker, Docker containers are invoked in a certain way for training and in a slightly different way for hosting. The following sections describe how to create containers for the SageMaker environment.\n",
    "\n",
    "\n",
    "### How Amazon SageMaker runs the Docker container\n",
    "\n",
    "Because it can run the same image in training or hosting, Amazon SageMaker runs the container with the `train` or` serve` argument. How your container processes this argument depends on the container:\n",
    "\n",
    "* In the example here, we did not define an ʻENTRYPOINT ʻin the Dockerfile for Docker to execute the `train` command at training time and` serve` at service time. In this example, we define them as executable Python scripts, but they could be any program that we want to start in that environment.\n",
    "* If you specify a program as \"ENTRYPOINT\" in the Dockerfile, that program will run at startup and its first argument will be either `train` or` serve`. The program can then examine that argument and decide what to do.\n",
    "* If you are building separate containers for training and hosting (or building just for one or the other), you can define a program as \"ENTRYPOINT\" in the Dockerfile and ignore (or check) the first argument passed.\n",
    "\n",
    "#### Run container during training\n",
    "\n",
    "When Amazon SageMaker runs the training, the `train` script runs like a regular Python program. A series of files are arranged for your use, under the `/ opt / ml` directory:\n",
    "\n",
    "    /opt/ml\n",
    "    ├── input\n",
    "    │   ├── config\n",
    "    │   │   ├── hyperparameters.json\n",
    "    │   │   └── resourceConfig.json\n",
    "    │   └── data\n",
    "    │       └── <channel_name>\n",
    "    │           └── <input data>\n",
    "    ├── model\n",
    "    │   └── <model files>\n",
    "    └── output\n",
    "        └── failure\n",
    "\n",
    "\n",
    "##### The entrance\n",
    "\n",
    "* `/ opt / ml / input / config` contains information to control how the program runs. `hyperparameters.json` is a JSON-formatted dictionary of hyperparameter names to values. These values ​​will always be strings, so you may need to convert them. `ResourceConfig.json` is a JSON-formatted file that describes the network layout used for distributed training. Since scikit-learn does not support distributed training, we will ignore it here.\n",
    "* `/ opt / ml / input / data / <channel_name> /` (for File mode) contains the input data for that channel. Channels are created based on the call to CreateTrainingJob, but it is generally important that the channels match what the algorithm expects. The files for each channel will be copied from S3 to this directory, preserving the tree structure indicated by the S3 key structure.\n",
    "* `/ opt / ml / input / data / <channel_name> _ <epoch_number>` (for Pipe mode) is the pipe for a given epoch. The epochs start at zero and go up by one each time you read them. There is no limit to the number of epochs you can run, but you must close each pipe before reading the next epoch.\n",
    "    \n",
    "##### The exit\n",
    "\n",
    "* `/ opt / ml / model /` is the directory where the model generated by your algorithm is written. Your model can be in any format you want. It can be a single file or an entire directory tree. SagMaker will package any files in this directory into a compressed tar file. This file will be available in the S3 location returned in the `DescribeTrainingJob` output.\n",
    "* `/ opt / ml / output` is a directory where the algorithm can write a` failure` file that describes why the job failed. The content of this file will be returned in the `FailureReason` field of the` DescribeTrainingJob` result. For successful jobs, there is no reason to write this file as it will be ignored.\n",
    "\n",
    "#### Running the container during hosting\n",
    "\n",
    "Hosting has a very different model than training because it must respond to inference requests that arrive through HTTP. In this example, we use recommended Python code to provide a robust and scalable inference request service:\n",
    "\n",
    "Amazon SagMaker uses two URLs in the container:\n",
    "\n",
    "* `/ ping` will receive` GET` requests from the infrastructure. Returns 200 if the container is open and accepting requests.\n",
    "* `/ invocations` is the endpoint that receives inference` POST` requests from the client. The format of the request and the response depends on the algorithm. If the client supplied the `ContentType` and ʻccept` headers, these will also be passed.\n",
    "\n",
    "The container will have the model files in the same place where they were written during training:\n",
    "\n",
    "    /opt/ml\n",
    "    └── model\n",
    "        └── <model files>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Container Parts\n",
    "\n",
    "In the `container` directory are all the components you need to package the sample algorithm for Amazon SageManager:\n",
    "\n",
    "    .\n",
    "    ├── Dockerfile\n",
    "    ├── build_and_push.sh\n",
    "    └── decision_trees\n",
    "        ├── nginx.conf\n",
    "        ├── predictor.py\n",
    "        ├── serve\n",
    "        ├── train\n",
    "        └── wsgi.py\n",
    "\n",
    "\n",
    "Let's see each one:\n",
    "\n",
    "* __`Dockerfile`__ describes how to build the Docker container image. More details below.\n",
    "* __`build_and_push.sh`__ is a script that uses Dockerfile to build its container images and then publishes (push) it to ECR. We will invoke the commands directly later in this notebook, but you can copy and run the script for other algorithms.\n",
    "* __`prophet`__ is the directory that contains the files to be installed in the container.\n",
    "* __`local_test`__ is a directory that shows how to test the new container on any machine that can run Docker, including an Amazon SageMaker Notebook Instance. With this method, you can quickly iterate using small data sets to eliminate any structural errors before using the container with Amazon SageMaker.\n",
    "\n",
    "The files that we are going to put in the container are:\n",
    "\n",
    "* __`nginx.conf`__ is the configuration file for the nginx front-end. Generally, you should be able to take this file as is.\n",
    "* __`predictor.py`__ is the program that actually implements the Flask web server and Prophet predictions for this application.\n",
    "* __`serve`__ is the program started when the hosting container starts. It just launches the gunicorn server that runs multiple instances of the Flask application defined in `predictor.py`. You should be able to take this file as is.\n",
    "* __`train`__ is the program that is invoked when the container for training is executed.\n",
    "* __`wsgi.py`__ is a small wrapper used to invoke the Flask application. You should be able to take this file as is.\n",
    "\n",
    "In summary, the two Prophet-specific code files are `train` and` predictor.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dockerfile file\n",
    "\n",
    "The Dockerfile file describes the image we want to create. It is a description of the complete installation of the operating system of the system you want to run. A running Docker container is significantly lighter than a full operating system, however, because it leverages Linux on the host machine for basic operations.\n",
    "\n",
    "For this example, we will start from a standard Ubuntu installation and run the normal tools to install the things Prophet needs. Finally, we add the code that implements Prophet to the container and configure the correct environment to run correctly.\n",
    "\n",
    "The following is the Dockerfile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Build an image that can do training and inference in SageMaker\n",
      "# This is a Python 3 image that uses the nginx, gunicorn, flask stack\n",
      "# for serving inferences in a stable way.\n",
      "\n",
      "FROM ubuntu:16.04\n",
      "\n",
      "MAINTAINER Amazon AI <sage-learner@amazon.com>\n",
      "\n",
      "RUN apt-get -y update && apt-get install -y --no-install-recommends \\\n",
      "         wget \\\n",
      "         curl \\\n",
      "         python-dev \\\n",
      "         build-essential libssl-dev libffi-dev \\\n",
      "         libxml2-dev libxslt1-dev zlib1g-dev \\\n",
      "         nginx \\\n",
      "         ca-certificates \\\n",
      "    && rm -rf /var/lib/apt/lists/*\n",
      "\n",
      "RUN curl -fSsL -O https://bootstrap.pypa.io/get-pip.py && \\\n",
      "    python get-pip.py && \\\n",
      "    rm get-pip.py\n",
      " \n",
      "RUN pip --no-cache-dir install \\\n",
      "        numpy \\\n",
      "        scipy \\\n",
      "        sklearn \\\n",
      "        pandas \\\n",
      "        flask \\\n",
      "        gevent \\\n",
      "        gunicorn \\\n",
      "        pystan \n",
      "\n",
      "RUN pip --no-cache-dir install \\\n",
      "        fbprophet \n",
      "        \n",
      "ENV PYTHONUNBUFFERED=TRUE\n",
      "ENV PYTHONDONTWRITEBYTECODE=TRUE\n",
      "ENV PATH=\"/opt/program:${PATH}\"\n",
      "\n",
      "# Set up the program in the image\n",
      "COPY prophet /opt/program\n",
      "WORKDIR /opt/program\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cat container/Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El archivo train\n",
    "\n",
    "The train file describes the way we are going to do the training.\n",
    "The Prophet-Docker / container / prophet / train file contains the specific training code for Prophet.\n",
    "We must modify the train () function in the following way:\n",
    "\n",
    "    def train():\n",
    "        print('Starting the training.')\n",
    "        try:\n",
    "            # Read in any hyperparameters that the user passed with the training job\n",
    "            with open(param_path, 'r') as tc:\n",
    "                trainingParams = json.load(tc)\n",
    "            # Take the set of files and read them all into a single pandas dataframe\n",
    "            input_files = [ os.path.join(training_path, file) for file in os.listdir(training_path) ]\n",
    "            if len(input_files) == 0:\n",
    "                raise ValueError(('There are no files in {}.\\n' +\n",
    "                                  'This usually indicates that the channel ({}) was incorrectly specified,\\n' +\n",
    "                                  'the data specification in S3 was incorrectly specified or the role specified\\n' +\n",
    "                                  'does not have permission to access the data.').format(training_path, channel_name))\n",
    "            raw_data = [ pd.read_csv(file, error_bad_lines=False ) for file in input_files ]\n",
    "            train_data = pd.concat(raw_data)\n",
    "            train_data.columns = ['ds', 'y']\n",
    "\n",
    "            # Usamos Prophet para entrenar el modelo.\n",
    "            clf = Prophet()\n",
    "            clf = clf.fit(train_data)\n",
    "\n",
    "            # save the model\n",
    "            with open(os.path.join(model_path, 'prophet-model.pkl'), 'w') as out:\n",
    "                pickle.dump(clf, out)\n",
    "            print('Training complete.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El archivo predictor.py\n",
    "\n",
    "The predictor.py file describes the way we are going to make predictions.\n",
    "The file Prophet-Docker / container / prophet / predictor.py contains the specific prediction code for Prophet.\n",
    "We must modify the predict () function in the following way:\n",
    "\n",
    "    def predict(cls, input):\n",
    "        \"\"\"For the input, do the predictions and return them.\n",
    "\n",
    "        Args:\n",
    "            input (a pandas dataframe): The data on which to do the predictions. There will be\n",
    "                one prediction per row in the dataframe\"\"\"\n",
    "        clf = cls.get_model()\n",
    "        future = clf.make_future_dataframe(periods=int(input.iloc[0]))\n",
    "        print(int(input.iloc[0]))\n",
    "        print(input)\n",
    "        forecast = clf.predict(future)\n",
    "              \n",
    "        return forecast.tail(int(input.iloc[0]))\n",
    "\n",
    "\n",
    "\n",
    "And then the transformation () function as follows:\n",
    "\n",
    "    def transformation():\n",
    "        \"\"\"Do an inference on a single batch of data. In this sample server, we take data as CSV, convert\n",
    "        it to a pandas data frame for internal use and then convert the predictions back to CSV (which really\n",
    "        just means one prediction per line, since there's a single column.\n",
    "        \"\"\"\n",
    "        data = None\n",
    "\n",
    "        # Convert from CSV to pandas\n",
    "        if flask.request.content_type == 'text/csv':\n",
    "            data = flask.request.data.decode('utf-8')\n",
    "            s = StringIO.StringIO(data)\n",
    "            data = pd.read_csv(s, header=None)\n",
    "        else:\n",
    "            return flask.Response(response='This predictor only supports CSV data', status=415, mimetype='text/plain')\n",
    "\n",
    "        print('Invoked with {} records'.format(data.shape[0]))\n",
    "\n",
    "        # Do the prediction\n",
    "        predictions = ScoringService.predict(data)\n",
    "\n",
    "        # Convert from numpy back to CSV\n",
    "        out = StringIO.StringIO()\n",
    "        pd.DataFrame({'results':[predictions]}, index=[0]).to_csv(out, header=False, index=False)\n",
    "        result = out.getvalue()\n",
    "\n",
    "        return flask.Response(response=result, status=200, mimetype='text/csv')\n",
    " \n",
    "\n",
    "Basically we modify the line:\n",
    "\n",
    "        pd.DataFrame({'results':predictions}).to_csv(out, header=False, index=False)\n",
    " \n",
    "By the line:\n",
    "\n",
    "        pd.DataFrame({'results':[predictions]}, index=[0]).to_csv(out, header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Part 3: Using Prophet in Amazon SageMaker\n",
    "Now that we have all the files created, we are going to use Prophet in Sagemaker\n",
    "\n",
    "## Container assembly\n",
    "We start by building and registering the container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Login Succeeded\n",
      "The push refers to repository [373578233960.dkr.ecr.ap-southeast-2.amazonaws.com/sagemaker-prophet]\n",
      "5f70bf18a086: Preparing\n",
      "fabe222a751f: Preparing\n",
      "436e06501221: Preparing\n",
      "a206157d69db: Preparing\n",
      "a0078ef23c21: Preparing\n",
      "555dcabb1b53: Preparing\n",
      "0a05481263d6: Preparing\n",
      "2948a9b7d240: Preparing\n",
      "9edaa71ce233: Preparing\n",
      "62fdddf6a67c: Preparing\n",
      "eff16de3ff64: Preparing\n",
      "61727f5e6796: Preparing\n",
      "555dcabb1b53: Waiting\n",
      "0a05481263d6: Waiting\n",
      "2948a9b7d240: Waiting\n",
      "9edaa71ce233: Waiting\n",
      "eff16de3ff64: Waiting\n",
      "61727f5e6796: Waiting\n",
      "62fdddf6a67c: Waiting\n",
      "a206157d69db: Layer already exists\n",
      "a0078ef23c21: Layer already exists\n",
      "5f70bf18a086: Layer already exists\n",
      "436e06501221: Layer already exists\n",
      "555dcabb1b53: Layer already exists\n",
      "2948a9b7d240: Layer already exists\n",
      "0a05481263d6: Layer already exists\n",
      "9edaa71ce233: Layer already exists\n",
      "62fdddf6a67c: Layer already exists\n",
      "eff16de3ff64: Layer already exists\n",
      "61727f5e6796: Layer already exists\n",
      "fabe222a751f: Pushed\n",
      "latest: digest: sha256:ebc759b931399f9c9ff8e810a6233c135d414c29712d5d0436ebe13f5e568754 size: 2832\n",
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "#2 [internal] load .dockerignore\n",
      "#2 transferring context: 2B 0.0s done\n",
      "#2 DONE 0.0s\n",
      "\n",
      "#1 [internal] load build definition from Dockerfile\n",
      "#1 transferring dockerfile: 40B 0.0s done\n",
      "#1 DONE 0.0s\n",
      "\n",
      "#3 [internal] load metadata for docker.io/library/ubuntu:16.04\n",
      "#3 DONE 2.5s\n",
      "\n",
      "#4 [1/9] FROM docker.io/library/ubuntu:16.04@sha256:bb69f1a2b6c840b01eeffef...\n",
      "#4 DONE 0.0s\n",
      "\n",
      "#11 [internal] load build context\n",
      "#11 transferring context: 3.37kB done\n",
      "#11 DONE 0.0s\n",
      "\n",
      "#8 [5/9] RUN curl -fSsL -O https://bootstrap.pypa.io/get-pip.py &&     pyth...\n",
      "#8 CACHED\n",
      "\n",
      "#9 [6/9] RUN pip --no-cache-dir install         numpy         scipy        ...\n",
      "#9 CACHED\n",
      "\n",
      "#5 [2/9] RUN apt-get update && apt-get install -y software-properties-commo...\n",
      "#5 CACHED\n",
      "\n",
      "#6 [3/9] RUN apt-get -y update && apt-get install -y --no-install-recommend...\n",
      "#6 CACHED\n",
      "\n",
      "#7 [4/9] RUN ln -sfn /usr/bin/python3.6 /usr/bin/python3 && ln -sfn /usr/bi...\n",
      "#7 CACHED\n",
      "\n",
      "#10 [7/9] RUN pip --no-cache-dir install         fbprophet\n",
      "#10 CACHED\n",
      "\n",
      "#12 [8/9] COPY prophet /opt/program\n",
      "#12 DONE 0.0s\n",
      "\n",
      "#13 [9/9] WORKDIR /opt/program\n",
      "#13 DONE 0.0s\n",
      "\n",
      "#14 exporting to image\n",
      "#14 exporting layers 0.0s done\n",
      "#14 writing image sha256:38d85051e73b1961d857d6a4d03ee3a3d79a79fcef5e1cae4ee8a2439c02f2e2\n",
      "#14 writing image sha256:38d85051e73b1961d857d6a4d03ee3a3d79a79fcef5e1cae4ee8a2439c02f2e2 done\n",
      "#14 naming to docker.io/library/sagemaker-prophet done\n",
      "#14 DONE 0.0s\n",
      "CPU times: user 5.8 ms, sys: 12 ms, total: 17.8 ms\n",
      "Wall time: 10.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%sh\n",
    "\n",
    "# The name of our algorithm\n",
    "algorithm_name=sagemaker-prophet\n",
    "\n",
    "cd container\n",
    "\n",
    "chmod +x prophet/train\n",
    "chmod +x prophet/serve\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "region=$(aws configure get region)\n",
    "region=${region:-us-west-2}\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "## Get the login command from ECR and execute it directly\n",
    "##$(aws ecr get-login --region ${region} --no-include-email)\n",
    "\n",
    "# Switched to new method of ecr login for docker\n",
    "aws ecr get-login-password --region ${region} | docker login --username AWS --password-stdin \"${account}.dkr.ecr.${region}.amazonaws.com\" \n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "\n",
    "# docker build  -t ${algorithm_name} .\n",
    "# docker tag ${algorithm_name} ${fullname}\n",
    "\n",
    "# docker push ${fullname}"
   ]
  },
  {
   "source": [
    "## Building the Training Environment\n",
    "We initialize the session, execution role."
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting sagemaker\n  Downloading https://files.pythonhosted.org/packages/17/46/6f33739ddb673583b9ff6b6bada62813a1326edfe3a9cebd5e2df4ab23a0/sagemaker-2.16.1.tar.gz (306kB)\nRequirement already satisfied: boto3>=1.14.12 in /usr/local/lib/python3.7/site-packages (from sagemaker) (1.14.47)\nCollecting google-pasta (from sagemaker)\n  Downloading https://files.pythonhosted.org/packages/a3/de/c648ef6835192e6e2cc03f40b19eeda4382c49b5bafb43d88b931c4c74ac/google_pasta-0.2.0-py3-none-any.whl (57kB)\nRequirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.7/site-packages (from sagemaker) (1.19.1)\nCollecting protobuf>=3.1 (from sagemaker)\n  Downloading https://files.pythonhosted.org/packages/9c/d2/10c325d657155ad7bd942f9d2652ae8e3e7a4be66723e9789099283733df/protobuf-3.13.0-cp37-cp37m-macosx_10_9_x86_64.whl (1.3MB)\nCollecting protobuf3-to-dict>=0.1.5 (from sagemaker)\n  Downloading https://files.pythonhosted.org/packages/6b/55/522bb43539fed463275ee803d79851faaebe86d17e7e3dbc89870d0322b9/protobuf3-to-dict-0.1.5.tar.gz\nCollecting smdebug-rulesconfig==0.1.5 (from sagemaker)\n  Downloading https://files.pythonhosted.org/packages/3e/d9/10f5d4e1992575b704a9a0d0af1e962e31c6409ab414f686fdc2e2d42787/smdebug_rulesconfig-0.1.5-py2.py3-none-any.whl\nCollecting importlib-metadata>=1.4.0 (from sagemaker)\n  Downloading https://files.pythonhosted.org/packages/6d/6d/f4bb28424bc677bce1210bc19f69a43efe823e294325606ead595211f93e/importlib_metadata-2.0.0-py2.py3-none-any.whl\nCollecting packaging>=20.0 (from sagemaker)\n  Using cached https://files.pythonhosted.org/packages/46/19/c5ab91b1b05cfe63cccd5cfc971db9214c6dd6ced54e33c30d5af1d2bc43/packaging-20.4-py2.py3-none-any.whl\nRequirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /Users/chorder/Library/Python/3.7/lib/python/site-packages (from boto3>=1.14.12->sagemaker) (0.3.3)\nRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /Users/chorder/Library/Python/3.7/lib/python/site-packages (from boto3>=1.14.12->sagemaker) (0.9.4)\nRequirement already satisfied: botocore<1.18.0,>=1.17.47 in /usr/local/lib/python3.7/site-packages (from boto3>=1.14.12->sagemaker) (1.17.47)\nRequirement already satisfied: six in /Users/chorder/Library/Python/3.7/lib/python/site-packages (from google-pasta->sagemaker) (1.12.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.7/site-packages (from protobuf>=3.1->sagemaker) (41.0.1)\nCollecting zipp>=0.5 (from importlib-metadata>=1.4.0->sagemaker)\n  Downloading https://files.pythonhosted.org/packages/41/ad/6a4f1a124b325618a7fb758b885b68ff7b058eec47d9220a12ab38d90b1f/zipp-3.4.0-py3-none-any.whl\nCollecting pyparsing>=2.0.2 (from packaging>=20.0->sagemaker)\n  Using cached https://files.pythonhosted.org/packages/8a/bb/488841f56197b13700afd5658fc279a2025a39e22449b7cf29864669b15d/pyparsing-2.4.7-py2.py3-none-any.whl\nRequirement already satisfied: docutils<0.16,>=0.10 in /Users/chorder/Library/Python/3.7/lib/python/site-packages (from botocore<1.18.0,>=1.17.47->boto3>=1.14.12->sagemaker) (0.15.2)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /Users/chorder/Library/Python/3.7/lib/python/site-packages (from botocore<1.18.0,>=1.17.47->boto3>=1.14.12->sagemaker) (2.8.0)\nRequirement already satisfied: urllib3<1.26,>=1.20; python_version != \"3.4\" in /Users/chorder/Library/Python/3.7/lib/python/site-packages (from botocore<1.18.0,>=1.17.47->boto3>=1.14.12->sagemaker) (1.25.6)\nBuilding wheels for collected packages: sagemaker, protobuf3-to-dict\n  Building wheel for sagemaker (setup.py): started\n  Building wheel for sagemaker (setup.py): finished with status 'done'\n  Stored in directory: /Users/chorder/Library/Caches/pip/wheels/16/0d/24/cbcab58191f1129d3159dabede8d5eb1bf55cfc42fddfb4271\n  Building wheel for protobuf3-to-dict (setup.py): started\n  Building wheel for protobuf3-to-dict (setup.py): finished with status 'done'\n  Stored in directory: /Users/chorder/Library/Caches/pip/wheels/37/42/d8/1609d310cabebc2cf60eca038fa2b0c8503412963734a6fc31\nSuccessfully built sagemaker protobuf3-to-dict\nInstalling collected packages: google-pasta, protobuf, protobuf3-to-dict, smdebug-rulesconfig, zipp, importlib-metadata, pyparsing, packaging, sagemaker\nSuccessfully installed google-pasta-0.2.0 importlib-metadata-2.0.0 packaging-20.4 protobuf-3.13.0 protobuf3-to-dict-0.1.5 pyparsing-2.4.7 sagemaker-2.16.1 smdebug-rulesconfig-0.1.5 zipp-3.4.0\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "pip3 install sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 776 ms, sys: 416 ms, total: 1.19 s\nWall time: 2.57 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import boto3\n",
    "import re\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "import sagemaker as sage\n",
    "from time import gmtime, strftime\n",
    "\n",
    "\n",
    "prefix = 'DEMO-prophet-byo'\n",
    "# role = get_execution_role()\n",
    "sess = sage.Session()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# We upload the data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'s3://sagemaker-ap-southeast-2-373578233960/DEMO-prophet-byo'"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "WORK_DIRECTORY = 'data'\n",
    "data_location = sess.upload_data(WORK_DIRECTORY, key_prefix=prefix)\n",
    "data_location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We train the model\n",
    "Using the data uploaded to S3, we train the model by raising an ml.c4.2xlarge instance.\n",
    "Sagemaker will leave the trained model in the / output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2020-11-17 04:30:20 Starting - Starting the training job...\n",
      "2020-11-17 04:30:23 Starting - Launching requested ML instances......\n",
      "2020-11-17 04:31:28 Starting - Preparing the instances for training...\n",
      "2020-11-17 04:32:19 Downloading - Downloading input data\n",
      "2020-11-17 04:32:19 Training - Downloading the training image......\n",
      "2020-11-17 04:33:22 Uploading - Uploading generated training model\n",
      "2020-11-17 04:33:22 Completed - Training job completed\n",
      "\u001b[34mImporting plotly failed. Interactive plots will not work.\u001b[0m\n",
      "\u001b[34mStarting the training.\u001b[0m\n",
      "\u001b[34m['/opt/ml/input/data/training/avocado_daily.csv']\u001b[0m\n",
      "\u001b[34mINFO:fbprophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\u001b[0m\n",
      "\u001b[34mINFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\u001b[0m\n",
      "\u001b[34mInitial log joint probability = -2.69053\n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      99         450.7    0.00449192       82.0517      0.5589      0.5589      133   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     199       451.775   2.58216e-07       68.3218           1           1      267   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     231       451.777   2.08006e-05       69.9606   3.181e-07       0.001      342  LS failed, Hessian reset \n",
      "     299        451.78   1.69453e-05       89.9386      0.7351      0.7351      427   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     358       452.645   0.000574755       70.2023   9.841e-06       0.001      531  LS failed, Hessian reset \n",
      "     399       452.827   0.000103108       65.3866           1           1      584   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     416       452.976   0.000381044       103.502   4.735e-06       0.001      674  LS failed, Hessian reset \n",
      "     478       453.044   1.11022e-08       68.9059      0.1788           1      765   \u001b[0m\n",
      "\u001b[34mOptimization terminated normally: \n",
      "  Convergence detected: relative gradient magnitude is below tolerance\u001b[0m\n",
      "\u001b[34mTraining complete.\u001b[0m\n",
      "Training seconds: 75\n",
      "Billable seconds: 75\n",
      "CPU times: user 640 ms, sys: 157 ms, total: 797 ms\n",
      "Wall time: 3min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "account = sess.boto_session.client('sts').get_caller_identity()['Account']\n",
    "region = sess.boto_session.region_name\n",
    "image = '{}.dkr.ecr.{}.amazonaws.com/sagemaker-prophet:latest'.format(account, region)\n",
    "\n",
    "tseries = sage.estimator.Estimator(image,\n",
    "                       'AmazonSageMaker-ExecutionRole-20180723T001212', \n",
    "                        1, \n",
    "                        'ml.c4.2xlarge',\n",
    "                       output_path=\"s3://{}/output\".format(sess.default_bucket()),\n",
    "                       sagemaker_session=sess)\n",
    "\n",
    "tseries.fit(data_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Endpoint assembly for inference\n",
    "Using the newly trained model, we create an endpoint for inference hosted on an ml.c4.2xlarge instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-------------!CPU times: user 392 ms, sys: 76.7 ms, total: 468 ms\n",
      "Wall time: 6min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sagemaker.predictor import csv_serializer\n",
    "predictor = tseries.deploy(1, 'ml.m4.xlarge', serializer=csv_serializer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference test\n",
    "Finally we ask the model to predict the sales for the next 30 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "b'\"            ds     trend  ...  multiplicative_terms_upper      yhat\\n169 2018-03-26  1.473474  ...                         0.0  1.397270\\n170 2018-03-27  1.473135  ...                         0.0  1.400523\\n171 2018-03-28  1.472796  ...                         0.0  1.403892\\n172 2018-03-29  1.472458  ...                         0.0  1.407318\\n173 2018-03-30  1.472119  ...                         0.0  1.410742\\n174 2018-03-31  1.471781  ...                         0.0  1.414104\\n175 2018-04-01  1.471442  ...                         0.0  1.417342\\n176 2018-04-02  1.471103  ...                         0.0  1.420400\\n177 2018-04-03  1.470765  ...                         0.0  1.423223\\n178 2018-04-04  1.470426  ...                         0.0  1.425762\\n179 2018-04-05  1.470088  ...                         0.0  1.427975\\n180 2018-04-06  1.469749  ...                         0.0  1.429823\\n181 2018-04-07  1.469410  ...                         0.0  1.431280\\n182 2018-04-08  1.469072  ...                         0.0  1.432324\\n183 2018-04-09  1.468733  ...                         0.0  1.432942\\n184 2018-04-10  1.468395  ...                         0.0  1.433130\\n185 2018-04-11  1.468056  ...                         0.0  1.432894\\n186 2018-04-12  1.467717  ...                         0.0  1.432246\\n187 2018-04-13  1.467379  ...                         0.0  1.431206\\n188 2018-04-14  1.467040  ...                         0.0  1.429802\\n189 2018-04-15  1.466701  ...                         0.0  1.428068\\n190 2018-04-16  1.466363  ...                         0.0  1.426043\\n191 2018-04-17  1.466024  ...                         0.0  1.423770\\n192 2018-04-18  1.465686  ...                         0.0  1.421297\\n193 2018-04-19  1.465347  ...                         0.0  1.418673\\n194 2018-04-20  1.465008  ...                         0.0  1.415948\\n195 2018-04-21  1.464670  ...                         0.0  1.413174\\n196 2018-04-22  1.464331  ...                         0.0  1.410400\\n197 2018-04-23  1.463993  ...                         0.0  1.407674\\n198 2018-04-24  1.463654  ...                         0.0  1.405043\\n\\n[30 rows x 16 columns]\"\\n'\n",
      "CPU times: user 26.6 ms, sys: 42.7 ms, total: 69.2 ms\n",
      "Wall time: 5.14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "p = predictor.predict(\"30\")\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.4 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}